{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95dc4aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should be exactly the same between all models\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from common import *\n",
    "from util import *\n",
    "\n",
    "# Load everything\n",
    "scaled_data = {}\n",
    "with np.load(PREPROCESSED_DATASET_FILEPATH) as npz_loader:\n",
    "    for key in npz_loader:\n",
    "        scaled_data[key] = npz_loader[key]\n",
    "scaler = pk.load(open(PREPROCESSING_SCALER_FILEPATH, \"rb\"))\n",
    "\n",
    "# Input and output dims\n",
    "input_shape = tuple(list(scaled_data['x_train'].shape)[1:])\n",
    "output_shape = tuple(list(scaled_data['y_train'].shape)[1:])\n",
    "input_dims = np.product(input_shape)\n",
    "output_dims = np.product(output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "096ed454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow: 2.0.0\n",
      "numpy: 1.17.0\n"
     ]
    }
   ],
   "source": [
    "print(\"tensorflow: {}\".format(tf.__version__))\n",
    "print(\"numpy: {}\".format(np.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e45cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS:\n",
    "MODEL_NAME = \"Feedforward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d2c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct TF model here\n",
    "\n",
    "# Simple Feedforward Neural Network\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=256,activation='relu',),\n",
    "        #kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "        #tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=256,activation='relu',),\n",
    "        #kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "        #tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=output_dims,activation='linear'),\n",
    "        tf.keras.layers.Reshape(output_shape)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c038497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters and callbacks\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MAX_EPOCHS = 20\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch<50:\n",
    "        return lr\n",
    "    elif epoch%20==0:\n",
    "        return lr/10\n",
    "    else:\n",
    "        return lr\n",
    "    \n",
    "callback_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "callback_es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3b2307e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23974 samples, validate on 3425 samples\n",
      "Epoch 1/20\n",
      "23974/23974 [==============================] - 16s 664us/sample - loss: 0.2496 - val_loss: 0.2223\n",
      "Epoch 2/20\n",
      "23974/23974 [==============================] - 14s 578us/sample - loss: 0.2045 - val_loss: 0.2130\n",
      "Epoch 3/20\n",
      "23974/23974 [==============================] - 14s 575us/sample - loss: 0.1956 - val_loss: 0.2045\n",
      "Epoch 4/20\n",
      "23974/23974 [==============================] - 14s 573us/sample - loss: 0.1881 - val_loss: 0.2090\n",
      "Epoch 5/20\n",
      "23974/23974 [==============================] - 14s 568us/sample - loss: 0.1830 - val_loss: 0.1998\n",
      "Epoch 6/20\n",
      "23974/23974 [==============================] - 14s 565us/sample - loss: 0.1788 - val_loss: 0.1974\n",
      "Epoch 7/20\n",
      "23974/23974 [==============================] - 14s 576us/sample - loss: 0.1752 - val_loss: 0.1933\n",
      "Epoch 8/20\n",
      "23974/23974 [==============================] - 14s 576us/sample - loss: 0.1729 - val_loss: 0.1978\n",
      "Epoch 9/20\n",
      "23974/23974 [==============================] - 14s 574us/sample - loss: 0.1714 - val_loss: 0.1967\n",
      "Epoch 10/20\n",
      "23974/23974 [==============================] - 14s 571us/sample - loss: 0.1689 - val_loss: 0.1943\n",
      "Epoch 11/20\n",
      "23974/23974 [==============================] - 14s 578us/sample - loss: 0.1671 - val_loss: 0.1954\n",
      "Epoch 12/20\n",
      "23974/23974 [==============================] - 14s 565us/sample - loss: 0.1669 - val_loss: 0.2062\n",
      "Epoch 13/20\n",
      "23974/23974 [==============================] - 14s 574us/sample - loss: 0.1671 - val_loss: 0.1911\n",
      "Epoch 14/20\n",
      "23974/23974 [==============================] - 14s 579us/sample - loss: 0.1654 - val_loss: 0.1915\n",
      "Epoch 15/20\n",
      "23974/23974 [==============================] - 14s 575us/sample - loss: 0.1667 - val_loss: 0.1962\n",
      "Epoch 16/20\n",
      "23974/23974 [==============================] - 14s 567us/sample - loss: 0.1639 - val_loss: 0.1948\n",
      "Epoch 17/20\n",
      "23974/23974 [==============================] - 14s 572us/sample - loss: 0.1625 - val_loss: 0.1894\n",
      "Epoch 18/20\n",
      "23974/23974 [==============================] - 14s 572us/sample - loss: 0.1631 - val_loss: 0.1945\n",
      "Epoch 19/20\n",
      "23974/23974 [==============================] - 15s 616us/sample - loss: 0.1625 - val_loss: 0.1937\n",
      "Epoch 20/20\n",
      "23974/23974 [==============================] - 14s 567us/sample - loss: 0.1602 - val_loss: 0.1927\n"
     ]
    }
   ],
   "source": [
    "# Compile and fit model here\n",
    "history = model.fit(\n",
    "    x=scaled_data['x_train'],\n",
    "    y=scaled_data['y_train'],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=MAX_EPOCHS,\n",
    "    validation_data=(scaled_data['x_val'],\n",
    "    scaled_data['y_val']),\n",
    "    callbacks=[callback_lr,callback_es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e2aa55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute unnormalized prediction loss\n",
    "\n",
    "preds = {}\n",
    "\n",
    "for split in ['test','val']:\n",
    "    preds[split] = model.predict(scaled_data['x_'+split])\n",
    "    preds[split] = scaler.inverse_transform(preds[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b32541d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(model_name_to_model_filepath(MODEL_NAME))\n",
    "\n",
    "# Save run info\n",
    "run_info = {}\n",
    "run_info[\"history\"] = history.history\n",
    "run_info[\"predictions\"] = preds # idk whether this part makes sense for RNNs or not\n",
    "pk.dump(run_info, open(model_name_to_run_info_filepath(MODEL_NAME), \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
