{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95dc4aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should be exactly the same between all models\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from common import *\n",
    "from util import *\n",
    "\n",
    "# Load everything\n",
    "scaled_data = {}\n",
    "with np.load(PREPROCESSED_DATASET_FILEPATH) as npz_loader:\n",
    "    for key in npz_loader:\n",
    "        scaled_data[key] = npz_loader[key]\n",
    "scaler = pk.load(open(PREPROCESSING_SCALER_FILEPATH, \"rb\"))\n",
    "\n",
    "# Input and output dims\n",
    "input_shape = tuple(list(scaled_data['x_train'].shape)[1:])\n",
    "output_shape = tuple(list(scaled_data['y_train'].shape)[1:])\n",
    "input_dims = np.product(input_shape)\n",
    "output_dims = np.product(output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "096ed454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow: 2.0.0\n",
      "numpy: 1.17.0\n"
     ]
    }
   ],
   "source": [
    "print(\"tensorflow: {}\".format(tf.__version__))\n",
    "print(\"numpy: {}\".format(np.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e45cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS:\n",
    "MODEL_NAME = \"LinearRegression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d2c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct TF model here\n",
    "\n",
    "# Linear Regression\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=output_dims,activation='linear'),\n",
    "        tf.keras.layers.Reshape(output_shape)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c038497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters and callbacks\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MAX_EPOCHS = 20\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch<50:\n",
    "        return lr\n",
    "    elif epoch%20==0:\n",
    "        return lr/10\n",
    "    else:\n",
    "        return lr\n",
    "    \n",
    "callback_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "callback_es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3b2307e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23974 samples, validate on 3425 samples\n",
      "Epoch 1/20\n",
      "23974/23974 [==============================] - 76s 3ms/sample - loss: 0.3934 - val_loss: 0.3915\n",
      "Epoch 2/20\n",
      "23974/23974 [==============================] - 73s 3ms/sample - loss: 0.3395 - val_loss: 0.3550\n",
      "Epoch 3/20\n",
      "23974/23974 [==============================] - 71s 3ms/sample - loss: 0.3293 - val_loss: 0.3639\n",
      "Epoch 4/20\n",
      "23974/23974 [==============================] - 71s 3ms/sample - loss: 0.3276 - val_loss: 0.3418\n",
      "Epoch 5/20\n",
      "23974/23974 [==============================] - 71s 3ms/sample - loss: 0.3229 - val_loss: 0.3296\n",
      "Epoch 6/20\n",
      "23974/23974 [==============================] - 73s 3ms/sample - loss: 0.3218 - val_loss: 0.3271\n",
      "Epoch 7/20\n",
      "23974/23974 [==============================] - 75s 3ms/sample - loss: 0.3162 - val_loss: 0.3231\n",
      "Epoch 8/20\n",
      "23974/23974 [==============================] - 74s 3ms/sample - loss: 0.3159 - val_loss: 0.3239\n",
      "Epoch 9/20\n",
      "23974/23974 [==============================] - 76s 3ms/sample - loss: 0.3165 - val_loss: 0.3347\n",
      "Epoch 10/20\n",
      "23974/23974 [==============================] - 77s 3ms/sample - loss: 0.3128 - val_loss: 0.3354\n",
      "Epoch 11/20\n",
      "23974/23974 [==============================] - 76s 3ms/sample - loss: 0.3148 - val_loss: 0.3244\n",
      "Epoch 12/20\n",
      "23974/23974 [==============================] - 78s 3ms/sample - loss: 0.3132 - val_loss: 0.3164\n",
      "Epoch 13/20\n",
      "23974/23974 [==============================] - 76s 3ms/sample - loss: 0.3193 - val_loss: 0.3268\n",
      "Epoch 14/20\n",
      "23974/23974 [==============================] - 77s 3ms/sample - loss: 0.3151 - val_loss: 0.3301\n",
      "Epoch 15/20\n",
      "23974/23974 [==============================] - 78s 3ms/sample - loss: 0.3165 - val_loss: 0.3486\n",
      "Epoch 16/20\n",
      "23974/23974 [==============================] - 76s 3ms/sample - loss: 0.3153 - val_loss: 0.3184\n",
      "Epoch 17/20\n",
      "23974/23974 [==============================] - 69s 3ms/sample - loss: 0.3160 - val_loss: 0.3481\n",
      "Epoch 18/20\n",
      "23974/23974 [==============================] - 68s 3ms/sample - loss: 0.3131 - val_loss: 0.3287\n",
      "Epoch 19/20\n",
      "23974/23974 [==============================] - 69s 3ms/sample - loss: 0.3085 - val_loss: 0.3195\n",
      "Epoch 20/20\n",
      "23974/23974 [==============================] - 71s 3ms/sample - loss: 0.3169 - val_loss: 0.3552\n"
     ]
    }
   ],
   "source": [
    "# Compile and fit model here\n",
    "history = model.fit(\n",
    "    x=scaled_data['x_train'],\n",
    "    y=scaled_data['y_train'],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=MAX_EPOCHS,\n",
    "    validation_data=(scaled_data['x_val'],\n",
    "    scaled_data['y_val']),\n",
    "    callbacks=[callback_lr,callback_es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88fe3e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute unnormalized prediction loss\n",
    "\n",
    "preds = {}\n",
    "\n",
    "for split in ['test','val']:\n",
    "    preds[split] = model.predict(scaled_data['x_'+split])\n",
    "    preds[split] = scaler.inverse_transform(preds[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b32541d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(model_name_to_model_filepath(MODEL_NAME))\n",
    "\n",
    "# Save run info\n",
    "run_info = {}\n",
    "run_info[\"history\"] = history.history\n",
    "run_info[\"predictions\"] = preds # idk whether this part makes sense for RNNs or not\n",
    "pk.dump(run_info, open(model_name_to_run_info_filepath(MODEL_NAME), \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
